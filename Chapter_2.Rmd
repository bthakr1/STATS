---
title: "Data and Sampling Distribution"
author: "Bhupendrasinh Thakre"
date: "3/21/2020"
output: html_document
---
*Sample* : Asubset of data from larger population.

*Random Sampling* : Drawing elements into a sample at random. Each available member of the population being sampled
has an eqqual chance of being chosen for the sample. 

*with replacement* : Samples are drawn with replacement.

*without replacement* : Samples are drawn without replacement. 

```{r}

library(dplyr)
library(psych)

x <- 1:12

sample(x)

#With replace 

sample(x,replace = TRUE)

# With Probability

barplot(table(sample(1:3,100,replace = TRUE, prob = c(0.25,0.25,0.50))))

# Bernoulli's Trials

sample(c(0,1),10, replace = TRUE)

# random sampling data frame (3 rows)

sampled_mtcars = mtcars[sample(nrow(mtcars), 3, replace = TRUE),]

sampled_mtcars

# random sampling using dplyr

sampled_mtcars_dplyr = sample_n(mtcars, 5)

sampled_mtcars_dplyr

```

**Bias** : Refers to measurement or sampling errors that are *systematic* and *produced* by the measurement or sampling process. 

Distinction should be made between following two kind of errors. 

1. Error due to random chance.
2. Error due to bias. 

*An unbiased* process will produce error but it is *random* and does not tend strongly in any direction. 

*Sample Mean vs. Population Mean*

$\overline{x}$ --> Mean of sample from a population

$\mu$ --> Mean of a population

*Selection Bias* : Practice of selectively choosing data - consciously or unconsciously - in a way that leads to conclusion that is misleading or ephemeral. 

*Vast Search Effect* : If you repeatedly run different models and ask different questions with a large data set, you are bound to find 
something interesting. This can be removed with *holdout set* and *target schuffling*. 

*Regression to the mean* : Extreme observations tend to be followed by more central ones. It is the consequence of  particular 
form of "selection bias". 

**Specifying a hypothesis , the collectiong data following randomization and random sampling principles, ensures against bias**

Sampling Distibution : Refers to the distribution of some sample statistics , over many samples drawn from the same population. 

*****

Sampling Distribution : Frequency distribution of *sample statistics* over many samples or resamples.

Central Limit Theorem : Tendency of the sampling distibution to take on a normal shape as sample size rises.

Standard Error : Variability of a sample *statistcs* over many samples (not to be confused with *standard deviation* , which, by iteself refers to variability of **individual data values**)


*****

```{r}

#get meand for variables in data frame 
# excluding missing values
sapply(mtcars, mean, na.rm = TRUE)
# For median
sapply(mtcars, median, na.rm = TRUE)
# using psych package
describe(mtcars)

```



```{r}

loan_data <- read.csv("https://raw.githubusercontent.com/bthakr1/STATS/master/loan_data.csv")

head(loan_data)

nrow(loan_data)

```

```{r}

library(ggplot2)

# Take a random sample

samp_data <- data.frame(loan_amnt=sample(loan_data, 1000, replace = TRUE), type='data_dist')

head(samp_data)
```


*Standard Error* : Sums up the variability in the sampling distibution for a statistcs. As the sample size increases, SE decreases. 

Numerator : s (standard Deviation)

Denominator : square root of sample size or n

```{r}

x <- c(1,2.3,3,4.5,5,5.5,5.6,5.7,19)

se = sd(x)/sqrt(length(x))

se


```

*Bootstrap*

To estimate the sampling distribution of a statistics, or of model parameters, is to draw additional samples, with replacements, from the sample itself and recalculate the statistics or model for each resample.

Does not involve any assumption about the underlying data.


```{r}

library(boot)

hsb2 <- read.table("https://stats.idre.ucla.edu/stat/data/hsb2.csv", sep=",", header=T)

fc <- function(d,i){
  d2 <- d[i,]
  return(cor(d2$write,d2$math))
}

#bootcorr <- boot(hsb2,fc,R=500)

#bootcorr

rsq <- function(formula, data , indices){
  d <- data[indices,]
  fit <- lm(formula,data = d)
  return(summary(fit)$r.square)
}

results <- boot(data = mtcars, statistic = rsq, R = 1000, formula = mpg~wt+disp)

results

plot(results)

boot.ci(results, type="bca")


```

*Bagging* : Process of running multiple trees (classification and regression) on bootstrap samples and then averaging their predictions. 
Generally performs better than using a single tree.

Reduces the variance of an individual base learner. 

**Confidence Interval**

Example : An 90% CI around a sample estimate should, on average, contain similar sample estimate 90% of the time.

One more Example : An 75 % CI around a sample estimate should, on average, contain similar sample estimate 75 % of the time. 


```{r}

x <- sample.int(10000,10)

# assuming x is normally distributed with unknown mean and sd of 3140

# Find 90% and 95% CI for mean

# For 90 % CI 
# Alpha will be 1-0.90 = 0.10
# z(alpha/2) = z(0.05)
# qnorm(0.95)

# margin of error will be
# qnorm(0.95)*(SD/sqrt(n))

margin_of_error <- qnorm(0.95)*(sd(x)/sqrt(length(x)))

# lower bound of mean
lb_90 = mean(x) - margin_of_error

#upper bound of mean
ub_90 = mean(x) + margin_of_error

cat("The 90% Upper bound on mean is :", lb_90 ) 

cat(" And ")

cat("The 90% Lower bound on mean is :",  ub_90)


#For 95 % CI
# Alpha will be 1-.95 = 0.05
# z(alpha/2) = z(0.025)
#qnorm(0.975)

# margin of error for 95% CI
me <- qnorm(0.0975)*(sd(x)/sqrt(length(x)))


# lower bound on mean
lb = mean(x) - me
# upper bound on mean
ub = mean(x) + me

cat("The 95% Upper bound on mean is :", lb , "The 95% Lower bound on mean is :",  ub)



```























